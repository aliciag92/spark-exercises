{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "#starting spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n group\n",
       "0    0     c\n",
       "1    1     b\n",
       "2    2     c\n",
       "3    3     c\n",
       "4    4     a\n",
       "5    5     c\n",
       "6    6     c\n",
       "7    7     b\n",
       "8    8     c\n",
       "9    9     b\n",
       "10  10     c\n",
       "11  11     b\n",
       "12  12     a\n",
       "13  13     b\n",
       "14  14     c\n",
       "15  15     b\n",
       "16  16     a\n",
       "17  17     c\n",
       "18  18     a\n",
       "19  19     b"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "pd_df = pd.DataFrame(dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20)))\n",
    "\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert pd df to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    c|\n",
      "|  1|    b|\n",
      "+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pd_df)\n",
    "\n",
    "#must run .show() to see the spark dataframe \n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|                n|group|\n",
      "+-------+-----------------+-----+\n",
      "|  count|               20|   20|\n",
      "|   mean|              9.5| null|\n",
      "| stddev|5.916079783099616| null|\n",
      "|    min|                0|    a|\n",
      "|    max|               19|    c|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "#create spark df from pydataset mpg dataset\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Columns\n",
    "\n",
    "#### This returns a column object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use *.select()* to select multiple column objects. \n",
    "\n",
    "\n",
    "\n",
    "#### To select the values in the column object, we follow it with *.show()*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|hwy|cty|model|\n",
      "+---+---+-----+\n",
      "| 29| 18|   a4|\n",
      "| 29| 21|   a4|\n",
      "+---+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select 3 columns and show 2 rows\n",
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "+---+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select 1 column, \n",
    "#then select that column and add one to each of the values, \n",
    "#return and show both columns. \n",
    "mpg.select(mpg.hwy, mpg.hwy + 1).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|highway_mileage|\n",
      "+---------------+\n",
      "|             29|\n",
      "|             29|\n",
      "+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select & alias hwy column name\n",
    "mpg.select(mpg.hwy.alias(\"highway_mileage\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "+---------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a var col1 to store the column object of hwy, aliased as highway_mileage\n",
    "col1 = mpg.hwy.alias(\"highway_mileage\")\n",
    "\n",
    "# create a var col2 to store the column object of hwy divided by 2, aliased as highway_mileage_halved\n",
    "col2 = (mpg.hwy/2).alias(\"highway_mileage_halved\")\n",
    "\n",
    "# select both, referencing the new variables, col1 and col2\n",
    "mpg.select(col1, col2).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_df.col1, pd_df['col1'] --> pandas Series\n",
    "# mpg.hwy, col(\"hwy\") --> spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+\n",
      "|highway_mileage|city_mileage|avg_mileage|\n",
      "+---------------+------------+-----------+\n",
      "|             29|          18|       23.5|\n",
      "|             29|          21|       25.0|\n",
      "+---------------+------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add 2 cols together and divide by 2 to get avg col\n",
    "avg_col = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "\n",
    "#alias all cols\n",
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"highway_mileage\"),\n",
    "    mpg.cty.alias(\"city_mileage\"),\n",
    "    avg_col.alias(\"avg_mileage\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to do what we did above, using *.expr() *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-------------------+\n",
      "|hwy|(hwy + 1)|highway_mileage|highway_incremented|\n",
      "+---+---------+---------------+-------------------+\n",
      "| 29|       30|             29|                 30|\n",
      "| 29|       30|             29|                 30|\n",
      "| 31|       32|             31|                 32|\n",
      "| 30|       31|             30|                 31|\n",
      "| 26|       27|             26|                 27|\n",
      "+---+---------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    expr(\"hwy\"),  # the same as `col`\n",
    "    expr(\"hwy + 1\"),  # an arithmetic expression\n",
    "    expr(\"hwy AS highway_mileage\"),  # using an alias\n",
    "    expr(\"hwy + 1 AS highway_incremented\"),  # a combination of the above\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing together all the different ways to accomplish the same task...select a column & alias it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "|     30|     30|     30|     30|\n",
      "|     26|     26|     26|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the table with spark\n",
    "mpg.createOrReplaceTempView(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "+---+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run sql query\n",
    "#allows us to run SQL commands to manipulate and access data\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "    FROM mpg\n",
    "    \"\"\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change datatype\n",
    "#printShema to double check dtype\n",
    "mpg.select(mpg.hwy.cast(\"string\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|model|\n",
      "+-----+-----+\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shows null because can't be converted. \n",
    "mpg.select(mpg.model, mpg.model.cast(\"int\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg and mean are aliases of each other \n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean\n",
    "# from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+--------+--------+\n",
      "|(sum(hwy) / count(hwy) AS `average_1`)|        average_2|min(hwy)|max(hwy)|\n",
      "+--------------------------------------+-----------------+--------+--------+\n",
      "|                     23.44017094017094|23.44017094017094|      12|      44|\n",
      "+--------------------------------------+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    sum(mpg.hwy) / count(mpg.hwy).alias(\"average_1\"),\n",
    "    avg(mpg.hwy).alias(\"average_2\"),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#concat cols\n",
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function for string literals: lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|concat(cyl,  cylinders)|\n",
      "+-----------------------+\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            6 cylinders|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use lit() for space characters or other word characters\n",
    "mpg.select(concat(mpg.cyl, lit(\" cylinders\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating spark df from pd dataframe\n",
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using regexp_extract - extract at least one capture group and create new column of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    #select address, extract address, capture group start w/ and 1 or more digits, and just return 1st capture group\n",
    "    regexp_extract(\"address\", r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "    #after start of string, digits, and space, give me everything up until the comma, and return 1st capture group\n",
    "    regexp_extract(\"address\", r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexp_replace lets us make substitutions based on a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    #replaces address col, from the start, everything up until the comma and space\n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Filtering with .filter and .where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|      model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 25| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 23| 29|  p|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|manual(m5)|  f| 26| 34|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "|       honda|      civic|  2.0|2008|  4|manual(m6)|  f| 21| 29|  p|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|  auto(l4)|  f| 19| 26|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|manual(m5)|  f| 19| 29|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|manual(m5)|  f| 20| 28|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|  auto(l4)|  f| 20| 27|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|  auto(l4)|  4| 21| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|  auto(l4)|  4| 19| 26|  r|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|  volkswagen| new beetle|  2.0|1999|  4|manual(m5)|  f| 21| 29|  r|subcompact|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#class is a reserved word (so has to be called in brackets)\n",
    "mpg.filter(mpg.cyl == 4).where(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditionals with When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|displ|engine_size|\n",
      "+-----+-----------+\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "|  2.0|     medium|\n",
      "|  2.8|     medium|\n",
      "|  2.8|     medium|\n",
      "|  3.1|      large|\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select display when if less than 2, assign it to small,\n",
    "#when less than 3, assign it to medium, \n",
    "#otherwise, give it large\n",
    "#and then alias all the when/otherwise functions.\n",
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (\n",
    "        when(mpg.displ < 2, \"small\")\n",
    "        .when(mpg.displ < 3, \"medium\")\n",
    "        .otherwise(\"large\")\n",
    "        .alias(\"engine_size\")\n",
    "    ),\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting & Ordering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|manufacturer|              model|displ|year|cyl|     trans|drv|cty|hwy| fl| class|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|        jeep| grand cherokee 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|manual(m6)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|        durango 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|  dakota pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|   chevrolet|    k1500 tahoe 4wd|  5.3|2008|  8|  auto(l4)|  4| 11| 14|  e|   suv|\n",
      "|        jeep| grand cherokee 4wd|  6.1|2008|  8|  auto(l5)|  4| 11| 14|  p|   suv|\n",
      "|       dodge|        durango 4wd|  5.9|1999|  8|  auto(l4)|  4| 11| 15|  r|   suv|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sorts ascending by default\n",
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.hwy.desc())\n",
    "# is the same as\n",
    "mpg.sort(col(\"hwy\").desc())\n",
    "# is the same as\n",
    "mpg.sort(desc(\"hwy\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 20| 27|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 20| 26|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|manual(m5)|  4| 18| 25|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 19| 25|  p|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|  auto(l4)|  4| 18| 24|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 18| 23|  p|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|manual(m5)|  4| 15| 20|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|  auto(l4)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.0|2008|  6|  auto(l5)|  4| 17| 22|  d|  suv|\n",
      "|      toyota|       4runner 4wd|  4.0|2008|  6|  auto(l5)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  4.0|1999|  6|  auto(l4)|  4| 15| 20|  r|  suv|\n",
      "|      nissan|    pathfinder 4wd|  4.0|2008|  6|  auto(l5)|  4| 14| 20|  p|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|manual(m5)|  4| 15| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  3.4|1999|  6|  auto(l4)|  4| 15| 19|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.7|2008|  6|  auto(l5)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  3.4|1999|  6|manual(m5)|  4| 15| 17|  r|  suv|\n",
      "|      nissan|    pathfinder 4wd|  3.3|1999|  6|  auto(l4)|  4| 14| 17|  r|  suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort multiple cols either asc or desc\n",
    "mpg.sort(desc(\"class\"), mpg.cyl.asc(), col(\"hwy\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping & Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fa16078d9a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#different ways to group by columns\n",
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(col(\"cyl\"))\n",
    "mpg.groupBy(\"cyl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|cyl|          avg(cty)|         avg(hwy)|\n",
      "+---+------------------+-----------------+\n",
      "|  6| 16.21518987341772|22.82278481012658|\n",
      "|  5|              20.5|            28.75|\n",
      "|  8|12.571428571428571|17.62857142857143|\n",
      "|  4|21.012345679012345|28.80246913580247|\n",
      "+---+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregate after groupby\n",
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "|cyl|     class|          avg(cty)|          avg(hwy)|\n",
      "+---+----------+------------------+------------------+\n",
      "|  5|   compact|              21.0|              29.0|\n",
      "|  5|subcompact|              20.0|              28.5|\n",
      "|  6|subcompact|              17.0|24.714285714285715|\n",
      "|  6|    pickup|              14.5|              17.9|\n",
      "|  4|subcompact|22.857142857142858| 30.80952380952381|\n",
      "|  8|       suv|12.131578947368421|16.789473684210527|\n",
      "|  8|    pickup|              11.8|              15.8|\n",
      "|  8|   midsize|              16.0|              24.0|\n",
      "|  4|   midsize|              20.5|           29.1875|\n",
      "|  8|   2seater|              15.4|              24.8|\n",
      "|  6|   compact|16.923076923076923|25.307692307692307|\n",
      "|  6|   minivan|              15.6|              22.2|\n",
      "|  4|   compact|            21.375|          29.46875|\n",
      "|  8|subcompact|              14.8|              21.6|\n",
      "|  6|   midsize|17.782608695652176| 26.26086956521739|\n",
      "|  4|   minivan|              18.0|              24.0|\n",
      "|  4|    pickup|              16.0|20.666666666666668|\n",
      "|  6|       suv|              14.5|              18.5|\n",
      "|  4|       suv|              18.0|             23.75|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby multiple cols\n",
    "mpg.groupBy(\"cyl\", \"class\").agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollup will do the same aggregations, but also include overall totals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here the null value in cyl indicates the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| cyl|count|\n",
      "+----+-----+\n",
      "|null|  234|\n",
      "|   4|   81|\n",
      "|   5|    4|\n",
      "|   6|   79|\n",
      "|   8|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").count().sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here the null value in cyl indicates the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| cyl|         avg(hwy)|\n",
      "+----+-----------------+\n",
      "|null|23.44017094017094|\n",
      "|   4|28.80246913580247|\n",
      "|   5|            28.75|\n",
      "|   6|22.82278481012658|\n",
      "|   8|17.62857142857143|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").agg(expr(\"avg(hwy)\")).sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here the null values in cyl and class is showing overall average.\n",
    "#### Then shows overall average for 4 cyl.\n",
    "#### Next null is for 5 cyl's overall average, then overall avg for 6 cyl,  then overall avg for 8 cyl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     class|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|null|      null| 23.44017094017094|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|   compact|          29.46875|\n",
      "|   4|   midsize|           29.1875|\n",
      "|   4|   minivan|              24.0|\n",
      "|   4|    pickup|20.666666666666668|\n",
      "|   4|subcompact| 30.80952380952381|\n",
      "|   4|       suv|             23.75|\n",
      "|   5|      null|             28.75|\n",
      "|   5|   compact|              29.0|\n",
      "|   5|subcompact|              28.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   6|   compact|25.307692307692307|\n",
      "|   6|   midsize| 26.26086956521739|\n",
      "|   6|   minivan|              22.2|\n",
      "|   6|    pickup|              17.9|\n",
      "|   6|subcompact|24.714285714285715|\n",
      "|   6|       suv|              18.5|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   8|   2seater|              24.8|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rollup w/ multiple columns, take mean hwy, and sort cyl and class\n",
    "mpg.rollup(\"cyl\", \"class\").mean(\"hwy\").sort(col(\"cyl\"), col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Crosstables & Pivot Tables\n",
    "\n",
    "#### Crosstab is a simple way to get counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+\n",
      "| class_cyl|  4|  5|  6|  8|\n",
      "+----------+---+---+---+---+\n",
      "|   midsize| 16|  0| 23|  2|\n",
      "|subcompact| 21|  2|  7|  5|\n",
      "|   2seater|  0|  0|  0|  5|\n",
      "|    pickup|  3|  0| 10| 20|\n",
      "|   minivan|  1|  0| 10|  0|\n",
      "|       suv|  8|  0| 16| 38|\n",
      "|   compact| 32|  2| 13|  0|\n",
      "+----------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#crosstab cyl by class\n",
    "mpg.crosstab(\"class\", \"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use pivot to compute different aggregations than count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby class (to get the row), and pivot on cyl (to get cols)\n",
    "#and agg method to get values\n",
    "mpg.groupby(\"class\").pivot(\"cyl\").mean(\"hwy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating df with missing values\n",
    "#wrap spark.createDataFrame around creation of pandas df to turn it into a spark df\n",
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drops across all nulls\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill nulls w/ 0\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select subset to fill w/ 0\n",
    "#leaves y cols w/ NaNs\n",
    "df.na.fill(0, subset=\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop a full subset (dropping only y nulls)\n",
    "#drops 1st and 6th row, but left rows where x was NaN\n",
    "df.na.drop(subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how is spark thinking about our df? \n",
    "mpg.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a single step above ^\n",
    "\n",
    "This one below shows another step after \"Scan ExistingRDD\", a \"Project\" that contains the names of the columns we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#150L, hwy#154L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are going to do a more advanced select calcluation, but this is still just a single step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(cast((cyl#150L + hwy#154L) as double) / 2.0) AS avg_mpg#1490]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(((mpg.cyl + mpg.hwy) / 2).alias(\"avg_mpg\")).explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our filter below is also a single step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(cyl#150L) AND (cyl#150L = 6))\n",
      "+- *(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#150L, hwy#154L]\n",
      "+- *(1) Filter (isnotnull(cyl#150L) AND (cyl#150L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adds a step\n",
    "mpg.select(\"cyl\", \"hwy\").filter(expr(\"cyl = 6\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#150L, hwy#154L]\n",
      "+- *(1) Filter (isnotnull(cyl#150L) AND (cyl#150L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#146,model#147,displ#148,year#149L,cyl#150L,trans#151,drv#152,cty#153L,hwy#154L,fl#155,class#156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#order of plan does not change, b/c spark does it in the most optimal way\n",
    "mpg.filter(expr(\"cyl = 6\")).select(\"cyl\", \"hwy\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More DF Manipulations\n",
    "\n",
    "For these examples, we'll be working with a dataset of observations of the weather in seattle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461 rows 6 columns\n"
     ]
    }
   ],
   "source": [
    "# print number of rows & columns\n",
    "print(weather.count(), \"rows\", len(weather.columns), \"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2012-01-01', '2015-12-31')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the date range of the dataset and assign values to min_date and max_date variables\n",
    "#.first() to get first item\n",
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| min(date)| max(date)|\n",
      "+----------+----------+\n",
      "|2012-01-01|2015-12-31|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#w/out .first(), just returns df w/ values\n",
    "weather.select(min(\"date\"), max(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----+-------+--------+\n",
      "|      date|precipitation|wind|weather|temp_avg|\n",
      "+----------+-------------+----+-------+--------+\n",
      "|2012-01-01|          0.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3| 6.1|   rain|     6.0|\n",
      "|2012-01-06|          2.5| 2.2|   rain|     3.5|\n",
      "+----------+-------------+----+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute temp average \n",
    "#withColumn ---> creating this new col from the expr\n",
    "#and then drop the old cols\n",
    "weather = weather.withColumn(\n",
    "    \"temp_avg\", expr(\"ROUND(temp_min + temp_max) / 2\")\n",
    ").drop(\"temp_max\", \"temp_min\")\n",
    "\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate total rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|    total_rainfall|\n",
      "+-----+------------------+\n",
      "|    1|465.99999999999994|\n",
      "|    2|             422.0|\n",
      "|    3|             606.2|\n",
      "|    4|             375.4|\n",
      "|    5|             207.5|\n",
      "|    6|             132.9|\n",
      "|    7|              48.2|\n",
      "|    8|             163.7|\n",
      "|    9|235.49999999999997|\n",
      "|   10|             503.4|\n",
      "|   11|             642.5|\n",
      "|   12|             622.7|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#new col: month, creating from month of date col\n",
    "#once created, then group by month, and aggregate by summing the precipitation and alias it\n",
    "#then sort by month ascending (default)\n",
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now take a look at the average temperature for each type of weather in December 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|weather|    avg(temp_avg)|\n",
      "+-------+-----------------+\n",
      "|    fog|7.555555555555555|\n",
      "|    sun|2.977272727272727|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter month and year, group by weather and get mean of temp_avg\n",
    "(\n",
    "    weather.filter(month(\"date\") == 12)\n",
    "    .filter(year(\"date\") == 2013)\n",
    "    .groupBy(\"weather\")\n",
    "    .agg(mean(\"temp_avg\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now find out how many days had freezing temperatures in each month of 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|month|no_of_days_with_freezing_temps|\n",
      "+-----+------------------------------+\n",
      "|    1|                             3|\n",
      "|    2|                             0|\n",
      "|    3|                             0|\n",
      "|    4|                             0|\n",
      "|    5|                             0|\n",
      "|    6|                             0|\n",
      "|    7|                             0|\n",
      "|    8|                             0|\n",
      "|    9|                             0|\n",
      "|   10|                             0|\n",
      "|   11|                             0|\n",
      "|   12|                             5|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter year to 2013\n",
    "#create freezing_temps col and it's going to be a bool of whether temp_avg is <=0, casted as int(so it'll be 1 or 0)\n",
    "#create month col\n",
    "#group by month col and sum freezing_temps and alias it\n",
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"freezing_temps\").alias(\"no_of_days_with_freezing_temps\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----+-------+--------+--------------+\n",
      "|      date|precipitation|wind|weather|temp_avg|freezing_temps|\n",
      "+----------+-------------+----+-------+--------+--------------+\n",
      "|2013-01-01|          0.0| 2.7|    sun|     1.0|             0|\n",
      "|2013-01-02|          0.0| 3.2|    sun|     2.5|             0|\n",
      "|2013-01-03|          4.1| 3.0|   rain|     2.5|             0|\n",
      "|2013-01-04|          2.5| 2.8|   rain|     6.0|             0|\n",
      "|2013-01-05|          3.0| 3.1|   rain|     5.5|             0|\n",
      "|2013-01-06|          2.0| 3.0|   rain|     5.0|             0|\n",
      "|2013-01-07|          2.3| 7.3|   rain|     7.0|             0|\n",
      "|2013-01-08|         16.3| 6.3|   rain|     8.5|             0|\n",
      "|2013-01-09|         38.4| 5.1|   rain|     6.0|             0|\n",
      "|2013-01-10|          0.3| 2.1|   snow|     1.5|             0|\n",
      "|2013-01-11|          0.0| 1.9|drizzle|     0.0|             1|\n",
      "|2013-01-12|          0.0| 2.0|    sun|    -0.5|             1|\n",
      "|2013-01-13|          0.0| 1.5|    sun|    -1.0|             1|\n",
      "|2013-01-14|          0.0| 1.3|    sun|     0.5|             0|\n",
      "|2013-01-15|          0.0| 2.3|    sun|     3.0|             0|\n",
      "|2013-01-16|          0.0| 1.8|drizzle|     1.0|             0|\n",
      "|2013-01-17|          0.0| 1.0|drizzle|     0.5|             0|\n",
      "|2013-01-18|          0.0| 1.3|drizzle|     1.0|             0|\n",
      "|2013-01-19|          0.0| 1.9|drizzle|     0.5|             0|\n",
      "|2013-01-20|          0.0| 2.1|drizzle|     1.5|             0|\n",
      "+----------+-------------+----+-------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#breakdown\n",
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    #.withColumn(\"month\", month(\"date\"))\n",
    "    #.groupBy(\"month\")\n",
    "    #.agg(sum(\"freezing_temps\").alias(\"no_of_days_with_freezing_temps\"))\n",
    "    #.sort(\"month\")\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One last example, let's calculate the average temperature for each quarter of each year:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|quarter|          temp_avg|\n",
      "+----+-------+------------------+\n",
      "|2012|      1| 5.587912087912088|\n",
      "|2012|      2|12.675824175824175|\n",
      "|2012|      3|            18.375|\n",
      "|2012|      4| 8.581521739130435|\n",
      "|2013|      1| 6.405555555555556|\n",
      "|2013|      2|14.505494505494505|\n",
      "|2013|      3| 19.47826086956522|\n",
      "|2013|      4| 8.032608695652174|\n",
      "|2014|      1| 7.205555555555556|\n",
      "|2014|      2|14.296703296703297|\n",
      "|2014|      3|19.858695652173914|\n",
      "|2014|      4|  9.88586956521739|\n",
      "|2015|      1| 8.972222222222221|\n",
      "|2015|      2|15.258241758241759|\n",
      "|2015|      3|19.407608695652176|\n",
      "|2015|      4| 8.956521739130435|\n",
      "+----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create new col quarter\n",
    "#create new col year\n",
    "#group by year and quarter and get mean temp avg and sort by both\n",
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\"))\n",
    "    .withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"year\", \"quarter\")\n",
    "    .agg(mean(\"temp_avg\").alias(\"temp_avg\"))\n",
    "    .sort(\"year\", \"quarter\")\n",
    "    .show()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We could use a pivot table instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-----+----+\n",
      "|year|   1|    2|    3|   4|\n",
      "+----+----+-----+-----+----+\n",
      "|2012|5.59|12.68|18.38|8.58|\n",
      "|2013|6.41|14.51|19.48|8.03|\n",
      "|2014|7.21| 14.3|19.86|9.89|\n",
      "|2015|8.97|15.26|19.41|8.96|\n",
      "+----+----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\")) #create quarter\n",
    "    .withColumn(\"year\", year(\"date\")) #create year\n",
    "    .groupBy(\"year\") #rows\n",
    "    .pivot(\"quarter\") #cols\n",
    "    .agg(expr(\"ROUND(MEAN(temp_avg), 2) AS temp_avg\")) #values avg of temp_avg, rounding to 2 decimals, and alias it\n",
    "    .sort(\"year\")\n",
    "    .show()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+\n",
      "|quarter| 2012| 2013| 2014| 2015|\n",
      "+-------+-----+-----+-----+-----+\n",
      "|      1| 5.59| 6.41| 7.21| 8.97|\n",
      "|      2|12.68|14.51| 14.3|15.26|\n",
      "|      3|18.38|19.48|19.86|19.41|\n",
      "|      4| 8.58| 8.03| 9.89| 8.96|\n",
      "+-------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#or change rows to quarter and cols to year instead\n",
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\")) #create quarter\n",
    "    .withColumn(\"year\", year(\"date\")) #create year\n",
    "    .groupBy(\"quarter\") #rows\n",
    "    .pivot(\"year\") #cols\n",
    "    .agg(expr(\"ROUND(MEAN(temp_avg), 2) AS temp_avg\")) #values avg of temp_avg, rounding to 2 decimals, and alias it\n",
    "    .sort(\"quarter\")\n",
    "    .show()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by creating some data that we can join together:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- users ---\n",
      "+---+-----+-------+\n",
      "| id| name|role_id|\n",
      "+---+-----+-------+\n",
      "|  1|  bob|    1.0|\n",
      "|  2|  joe|    2.0|\n",
      "|  3|sally|    3.0|\n",
      "|  4| adam|    3.0|\n",
      "|  5| jane|    NaN|\n",
      "|  6| mike|    NaN|\n",
      "+---+-----+-------+\n",
      "\n",
      "--- roles ---\n",
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|    admin|\n",
      "|  2|   author|\n",
      "|  3| reviewer|\n",
      "|  4|commenter|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two dataframes together, we'll need to call the .join method on one of them and supply the other as an argument. \n",
    "\n",
    "\n",
    "In addition, we'll need to supply the condition on which we are joining. \n",
    "\n",
    "\n",
    "\n",
    "In our case, we are joining where the role_id column on the users table is equal to the id column on the roles table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+--------+\n",
      "| id| name|role_id| id|    name|\n",
      "+---+-----+-------+---+--------+\n",
      "|  1|  bob|    1.0|  1|   admin|\n",
      "|  3|sally|    3.0|  3|reviewer|\n",
      "|  4| adam|    3.0|  3|reviewer|\n",
      "|  2|  joe|    2.0|  2|  author|\n",
      "+---+-----+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inner join - kept values that existed on both tables\n",
    "users.join(roles, on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, spark will perform an inner join, meaning that records from both dataframes will have a match with the other.\n",
    "\n",
    "\n",
    "We can also specify either a left or a right join, which will keep all of the records from either the left or right side, even if those records don't have a match with the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----+--------+\n",
      "| id| name|role_id|  id|    name|\n",
      "+---+-----+-------+----+--------+\n",
      "|  5| jane|    NaN|null|    null|\n",
      "|  6| mike|    NaN|null|    null|\n",
      "|  1|  bob|    1.0|   1|   admin|\n",
      "|  3|sally|    3.0|   3|reviewer|\n",
      "|  4| adam|    3.0|   3|reviewer|\n",
      "|  2|  joe|    2.0|   2|  author|\n",
      "+---+-----+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#include left table- users\n",
    "users.join(roles, on=users.role_id == roles.id, how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+---+---------+\n",
      "|  id| name|role_id| id|     name|\n",
      "+----+-----+-------+---+---------+\n",
      "|   1|  bob|    1.0|  1|    admin|\n",
      "|null| null|   null|  4|commenter|\n",
      "|   3|sally|    3.0|  3| reviewer|\n",
      "|   4| adam|    3.0|  3| reviewer|\n",
      "|   2|  joe|    2.0|  2|   author|\n",
      "+----+-----+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#includes right table- roles\n",
    "users.join(roles, on=users.role_id == roles.id, how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that examples above have a duplicate id column. There are several ways we could go about dealing with this:\n",
    "\n",
    "alias each dataframe + explicitly select columns after joining (this could also be implemented with spark SQL)\n",
    "rename duplicated columns before merging\n",
    "drop duplicated columns after the merge (.drop(right.id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will acquire and prepare the data we will use in the rest of this module.\n",
    "\n",
    "- Acquiring Data\n",
    "- Data Prep\n",
    "- Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * #import everything\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark lets us read data in from a variety of data sources using what it calls a DataFrameReader. We can access the read property of our spark object and then set various options and read from a data source.\n",
    "\n",
    "### Using Data Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"source.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to do above:\n",
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|source_id|     source_username|\n",
      "+---------+--------------------+\n",
      "|   100137|    Merlene Blodgett|\n",
      "|   103582|         Carmen Cura|\n",
      "|   106463|     Richard Sanchez|\n",
      "|   119403|      Betty De Hoyos|\n",
      "|   119555|      Socorro Quiara|\n",
      "|   119868| Michelle San Miguel|\n",
      "|   120752|      Eva T. Kleiber|\n",
      "|   124405|           Lori Lara|\n",
      "|   132408|       Leonard Silva|\n",
      "|   135723|        Amy Cardenas|\n",
      "|   136202|    Michelle Urrutia|\n",
      "|   136979|      Leticia Garcia|\n",
      "|   137943|    Pamela K. Baccus|\n",
      "|   138605|        Marisa Ozuna|\n",
      "|   138650|      Kimberly Green|\n",
      "|   138650|Kimberly Green-Woods|\n",
      "|   138793| Guadalupe Rodriguez|\n",
      "|   138810|       Tawona Martin|\n",
      "|   139342|     Jessica Mendoza|\n",
      "|   139344|        Isis Mendoza|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- source_username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different datatypes\n",
    "- StringType\n",
    "- DoubleType\n",
    "- IntegerType\n",
    "- LongType\n",
    "- ShortType\n",
    "- TimestampType\n",
    "- FloatType\n",
    "- DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "#importing above, means we can define our schema and set datatypes\n",
    "#lets us be sure about the structure of our data, and can significantly increase the speed of loading data \n",
    "#(inferring the schema can be a costly operation for large datasets).\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "        \n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "#read csv with schema variable from above\n",
    "spark.read.csv(\"source.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data\n",
    "A spark dataframe can be written to a local destination using the `.write` property. Several common output formats are:\n",
    "\n",
    "- `csv`: for writing to a local csv file(s)\n",
    "- `parquet`: Parquet is a very popular columnar storage format for Hadoop.\n",
    "- `json`: for writing to a local json file(s)\n",
    "- `jdbc`: for writing to a SQL database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "\n",
    "# write to json, mode=overwrite, in case it's been written there before\n",
    "mpg.write.json(\"mpg_json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "mpg.write.csv(\"mpg_csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = spark.read.csv(\"case.csv\", header=True, inferSchema=True)\n",
    "\n",
    "mpg.write.csv(\"mpg_csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " SLA_due_date         | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 1/1/18 0:46          \n",
      " case_closed_date     | 1/3/18 8:11          \n",
      " SLA_due_date         | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  GOLIAD RD, ... \n",
      " council_district     | 3                    \n",
      "-RECORD 2------------------------------------\n",
      " case_id              | 1014127334           \n",
      " case_opened_date     | 1/1/18 0:48          \n",
      " case_closed_date     | 1/2/18 7:57          \n",
      " SLA_due_date         | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -3.022337963         \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.320729167          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 102  PALFREY ST W... \n",
      " council_district     | 3                    \n",
      "-RECORD 3------------------------------------\n",
      " case_id              | 1014127335           \n",
      " case_opened_date     | 1/1/18 1:29          \n",
      " case_closed_date     | 1/2/18 8:13          \n",
      " SLA_due_date         | 1/17/18 8:30         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -15.01148148         \n",
      " case_closed          | YES                  \n",
      " dept_division        | Code Enforcement     \n",
      " service_request_type | Front Or Side Yar... \n",
      " SLA_days             | 16.29188657          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 114  LA GARDE ST,... \n",
      " council_district     | 3                    \n",
      "-RECORD 4------------------------------------\n",
      " case_id              | 1014127336           \n",
      " case_opened_date     | 1/1/18 1:34          \n",
      " case_closed_date     | 1/1/18 13:29         \n",
      " SLA_due_date         | 1/1/18 4:34          \n",
      " case_late            | YES                  \n",
      " num_days_late        | 0.37216435200000003  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Animal Cruelty(Cr... \n",
      " SLA_days             | 0.125                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 734  CLEARVIEW DR... \n",
      " council_district     | 7                    \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#take a look at data, clean it up by adding vertical=True\n",
    "cases.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withColumnRenamed(old_col_to_rename, new_col_renames_old_col)\n",
    "cases = cases.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")\n",
    "\n",
    "#hasn't performed the action yet, just reassigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- case_due_date: string (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#look at current dtypes\n",
    "cases.printSchema()\n",
    "\n",
    "#case_closed and case_late col are currently strings, let's change dtype to bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|     true|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#recreate case_closed col (same as og col)\n",
    "#expr case_closed == YES, returns true or false\n",
    "#do the same for case_late col\n",
    "cases = cases.withColumn(\"case_closed\", expr('case_closed==\"YES\"'))\\\n",
    "            .withColumn(\"case_late\", expr('case_late==\"YES\"'))\n",
    "\n",
    "cases.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- case_due_date: string (nullable = true)\n",
      " |-- case_late: boolean (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: boolean (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check dtype change to bool\n",
    "cases.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|council_district| count|\n",
      "+----------------+------+\n",
      "|               1|119309|\n",
      "|               6| 74095|\n",
      "|               3|102706|\n",
      "|               5|114609|\n",
      "|               9| 40916|\n",
      "|               4| 93778|\n",
      "|               8| 42345|\n",
      "|               7| 72445|\n",
      "|              10| 62926|\n",
      "|               2|114745|\n",
      "|               0|  3830|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#council district shows up as int, but it is actually a category, so needs to be a string\n",
    "cases.groupBy(\"council_district\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- case_due_date: string (nullable = true)\n",
      " |-- case_late: boolean (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: boolean (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withColumn overwrites og col if the same col name is used\n",
    "#if withColumn has new name, then it will create a whole new col\n",
    "cases = cases.withColumn('council_district', col('council_district').cast('string'))\n",
    "\n",
    "cases.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "|     1/1/18 1:34|    1/1/18 13:29|  1/1/18 4:34|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#date cols are string dtypes, need to be converted to date dtypes\n",
    "cases.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|   case_opened_date|   case_closed_date|      case_due_date|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|\n",
      "|2018-01-01 00:46:00|2018-01-03 08:11:00|2018-01-05 08:30:00|\n",
      "|2018-01-01 00:48:00|2018-01-02 07:57:00|2018-01-05 08:30:00|\n",
      "|2018-01-01 01:29:00|2018-01-02 08:13:00|2018-01-17 08:30:00|\n",
      "|2018-01-01 01:34:00|2018-01-01 13:29:00|2018-01-01 04:34:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#identify current format they are in from above\n",
    "fmt = 'M/d/yy H:mm'\n",
    "\n",
    "#recreate each col and convert to timestampe using current fmt that it is in\n",
    "cases = (\n",
    "    cases.withColumn('case_opened_date', to_timestamp('case_opened_date', fmt))\n",
    "    .withColumn('case_closed_date', to_timestamp('case_closed_date', fmt))\n",
    "    .withColumn('case_due_date', to_timestamp('case_due_date', fmt))\n",
    ")\n",
    "\n",
    "#take a look at it\n",
    "cases.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  EL PASO ST,...|\n",
      "|2215  GOLIAD RD, ...|\n",
      "|102  PALFREY ST W...|\n",
      "|114  LA GARDE ST,...|\n",
      "|734  CLEARVIEW DR...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's normalize address field\n",
    "cases.select('request_address').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case and trim to make sure there is no leading or trailing whitespace\n",
    "cases = cases.withColumn('request_address', trim(lower(cases.request_address)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  el paso st,...|\n",
      "|2215  goliad rd, ...|\n",
      "|102  palfrey st w...|\n",
      "|114  la garde st,...|\n",
      "|734  clearview dr...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check\n",
    "cases.select('request_address').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      num_days_late|      num_weeks_late|\n",
      "+-------------------+--------------------+\n",
      "| -998.5087616000001|        -142.6441088|\n",
      "|-2.0126041669999997|-0.28751488099999994|\n",
      "|       -3.022337963|-0.43176256614285713|\n",
      "|       -15.01148148| -2.1444973542857144|\n",
      "|0.37216435200000003|         0.053166336|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#number of days late can be converted to number of weeks late\n",
    "cases = cases.withColumn('num_weeks_late', expr('num_days_late / 7 AS num_weeks_late'))\n",
    "\n",
    "#take a look at change\n",
    "cases.select('num_days_late', 'num_weeks_late').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|council_district|\n",
      "+----------------+\n",
      "|             005|\n",
      "|             003|\n",
      "|             003|\n",
      "|             003|\n",
      "|             007|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add leading 0s to council district to fix format\n",
    "\n",
    "#converting back to int temporarily\n",
    "cases = cases.withColumn('council_district', col('council_district').cast('int'))\n",
    "\n",
    "#'%03d' means at least 3 digits, pad with 0s (fill leading digits w/ 0s)\n",
    "\n",
    "#converted council_district back to int temporarily, but the final output will be a string.\n",
    "cases = cases.withColumn(\n",
    "    \"council_district\",\n",
    "    format_string(\"%03d\", col(\"council_district\").cast(\"int\")),\n",
    ")\n",
    "\n",
    "cases.select(\"council_district\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format_string returns col back to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: timestamp (nullable = true)\n",
      " |-- case_closed_date: timestamp (nullable = true)\n",
      " |-- case_due_date: timestamp (nullable = true)\n",
      " |-- case_late: boolean (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: boolean (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: string (nullable = false)\n",
      " |-- num_weeks_late: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78228|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add zipcode by extracting it from the address\n",
    "#extract from rqst_address w/ regex digits at the end of line, 0 for no capture group\n",
    "cases = cases.withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "\n",
    "cases.select(\"zipcode\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|       true|2018-01-01 00:42:00|2018-01-01 12:29:00|    1231|             0|            0|\n",
      "|       true|2018-01-01 00:46:00|2018-01-03 08:11:00|    1231|             2|            2|\n",
      "|       true|2018-01-01 00:48:00|2018-01-02 07:57:00|    1231|             1|            1|\n",
      "|       true|2018-01-01 01:29:00|2018-01-02 08:13:00|    1231|             1|            1|\n",
      "|       true|2018-01-01 01:34:00|2018-01-01 13:29:00|    1231|             0|            0|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "|      false|2018-01-02 09:39:00|            null|    1230|          null|         1230|\n",
      "|      false|2018-01-02 10:49:00|            null|    1230|          null|         1230|\n",
      "|      false|2018-01-02 13:45:00|            null|    1230|          null|         1230|\n",
      "|      false|2018-01-02 14:09:00|            null|    1230|          null|         1230|\n",
      "|      false|2018-01-02 14:34:00|            null|    1230|          null|         1230|\n",
      "+-----------+-------------------+----------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add case age: How old the case is; the difference in days between when the case was opened and the current day\n",
    "#days_to_closed: The number of days between when the case was opened and when it was closed\n",
    "#case_lifetime: Number of days between when the case was opened and when it was closed, \n",
    "            #if the case is still open, the number of days since the case was opened\n",
    "\n",
    "cases = (\n",
    "    cases.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "cases.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"case_closed\")).show(5)\n",
    "\n",
    "cases.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"! case_closed\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#access dept.csv dataset (contains more info about various dpts)\n",
    "dept = spark.read.csv(\"dept.csv\", header=True, inferSchema=True)\n",
    "dept.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double checking what field to join on by looking at unique values\n",
    "#### Important not to assume to join just b/c columns are named the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----+\n",
      "|dept_division               |count|\n",
      "+----------------------------+-----+\n",
      "|Miscellaneous               |1    |\n",
      "|Solid Waste                 |1    |\n",
      "|Field Operations            |1    |\n",
      "|Streets                     |1    |\n",
      "|Waste Collection            |1    |\n",
      "|District 7                  |1    |\n",
      "|Code Enforcement (IntExp)   |1    |\n",
      "|District 10                 |1    |\n",
      "|Vector                      |1    |\n",
      "|Reservations                |1    |\n",
      "|Dangerous Premise           |1    |\n",
      "|311 Call Center             |1    |\n",
      "|Brush                       |1    |\n",
      "|Dangerous Premise (IntExp)  |1    |\n",
      "|Code Enforcement (Internal) |1    |\n",
      "|Traffic Engineering Design  |1    |\n",
      "|District 2                  |1    |\n",
      "|Signals                     |1    |\n",
      "|Engineering Division        |1    |\n",
      "|Director's Office Horizontal|1    |\n",
      "+----------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check unique values in dept\n",
    "dept.groupBy('dept_division').count().show(truncate = False)\n",
    "                                            #truncate false to show full value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|dept_division               |count |\n",
      "+----------------------------+------+\n",
      "|Miscellaneous               |45123 |\n",
      "|Solid Waste                 |813   |\n",
      "|Field Operations            |116915|\n",
      "|Streets                     |38510 |\n",
      "|Waste Collection            |215122|\n",
      "|District 7                  |2     |\n",
      "|Code Enforcement (IntExp)   |2189  |\n",
      "|District 10                 |2     |\n",
      "|Vector                      |538   |\n",
      "|Reservations                |2     |\n",
      "|Dangerous Premise           |15479 |\n",
      "|311 Call Center             |2849  |\n",
      "|Brush                       |18212 |\n",
      "|Dangerous Premise (IntExp)  |36    |\n",
      "|Traffic Engineering Design  |4334  |\n",
      "|Code Enforcement (Internal) |198   |\n",
      "|District 2                  |3     |\n",
      "|Signals                     |20700 |\n",
      "|Engineering Division        |1375  |\n",
      "|Director's Office Horizontal|515   |\n",
      "+----------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#double check unique values of dept_division in cases\n",
    "cases.groupBy('dept_division').count().show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 12:29:00  \n",
      " case_due_date        | 2020-09-26 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  el paso st,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 1231                 \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 2018-01-01 00:46:00  \n",
      " case_closed_date     | 2018-01-03 08:11:00  \n",
      " case_due_date        | 2018-01-05 08:30:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | true                 \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  goliad rd, ... \n",
      " council_district     | 003                  \n",
      " num_weeks_late       | -0.28751488099999994 \n",
      " zipcode              | 78223                \n",
      " case_age             | 1231                 \n",
      " days_to_closed       | 2                    \n",
      " case_lifetime        | 2                    \n",
      " department           | Trans & Cap Impro... \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join on dept_division\n",
    "cases = (\n",
    "    #left join on dept_division, b/c if some dpts that have no cases that is okay\n",
    "    #we want all the cases even if dpt does not show up\n",
    "    cases.join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    #reduces noise\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .drop(cases.dept_division) #cause there is standardized_dept_name\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\") #better readability\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")\n",
    "\n",
    "cases.show(2, vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70%, 20% and 10%\n",
    "train, validate, test = cases.randomSplit([0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588316"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168901"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84487"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
